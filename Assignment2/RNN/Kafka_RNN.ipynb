{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has  684782 chars, 76 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('data/pride_and_prejudice.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has ', data_size, 'chars,' ,vocab_size, 'unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'7': 0, '”': 1, 'o': 2, 'v': 3, '?': 4, '“': 5, ';': 6, ')': 7, ' ': 8, 'm': 9, 'Y': 10, '.': 11, 'R': 12, 'x': 13, 'a': 14, 't': 15, ',': 16, '3': 17, 'N': 18, 'w': 19, '6': 20, 'I': 21, 'V': 22, 'u': 23, 'S': 24, 'P': 25, 'h': 26, 'd': 27, 'C': 28, '1': 29, 'j': 30, '!': 31, 's': 32, 'H': 33, 'F': 34, '9': 35, 'e': 36, 'f': 37, 'L': 38, 'Z': 39, 'J': 40, \"'\": 41, ':': 42, 'g': 43, 'G': 44, 'O': 45, 'D': 46, 'n': 47, 'r': 48, 'l': 49, 'W': 50, '-': 51, 'q': 52, 'z': 53, 'B': 54, 'b': 55, '2': 56, '5': 57, '8': 58, '0': 59, 'U': 60, 'c': 61, 'M': 62, '4': 63, 'y': 64, '*': 65, 'T': 66, '\\n': 67, 'E': 68, '_': 69, '(': 70, 'i': 71, 'A': 72, 'k': 73, 'p': 74, 'K': 75}\n",
      "{0: '7', 1: '”', 2: 'o', 3: 'v', 4: '?', 5: '“', 6: ';', 7: ')', 8: ' ', 9: 'm', 10: 'Y', 11: '.', 12: 'R', 13: 'x', 14: 'a', 15: 't', 16: ',', 17: '3', 18: 'N', 19: 'w', 20: '6', 21: 'I', 22: 'V', 23: 'u', 24: 'S', 25: 'P', 26: 'h', 27: 'd', 28: 'C', 29: '1', 30: 'j', 31: '!', 32: 's', 33: 'H', 34: 'F', 35: '9', 36: 'e', 37: 'f', 38: 'L', 39: 'Z', 40: 'J', 41: \"'\", 42: ':', 43: 'g', 44: 'G', 45: 'O', 46: 'D', 47: 'n', 48: 'r', 49: 'l', 50: 'W', 51: '-', 52: 'q', 53: 'z', 54: 'B', 55: 'b', 56: '2', 57: '5', 58: '8', 59: '0', 60: 'U', 61: 'c', 62: 'M', 63: '4', 64: 'y', 65: '*', 66: 'T', 67: '\\n', 68: 'E', 69: '_', 70: '(', 71: 'i', 72: 'A', 73: 'k', 74: 'p', 75: 'K'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print (char_to_ix)\n",
    "print (ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print (vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 10\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  \"\"\"\n",
    "  #store our inputs, hidden states, outputs, and probability values\n",
    "  xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  #init loss as 0\n",
    "  loss = 0\n",
    "  # forward pass                                                                                                                                                                              \n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "    xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "  # backward pass: compute gradients going backwards    \n",
    "  #initalize vectors for gradient values for each set of weights \n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #output probabilities\n",
    "    dy = np.copy(ps[t])\n",
    "    #derive our first gradient\n",
    "    dy[targets[t]] -= 1 # backprop into y  \n",
    "    #compute output gradient -  output times hidden states transpose\n",
    "    #When we apply the transpose weight matrix,  \n",
    "    #we can think intuitively of this as moving the error backward\n",
    "    #through the network, giving us some sort of measure of the error \n",
    "    #at the output of the lth layer. \n",
    "    #output gradient\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    #derivative of output bias\n",
    "    dby += dy\n",
    "    #backpropagate!\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "    dbh += dhraw #derivative of hidden bias\n",
    "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "    dhnext = np.dot(Whh.T, dhraw) \n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " 8tRVn;Os,'rTC'5,m”_3quVF?'ARM6j..3_g1'6(qUG6)um-xHtucV);qg,e*vtK6P;i).s;y'F-OmnP“!l5Zs uPrne*OH)AhLMz”(nGZLH?3_:ukz0;?,.JgpV29pA'q!lZ.wPqs612“5Jb;_q* AHk*Soh-oor'WBAco8a\n",
      "dm8jJ0kA”,OrG0NL :B“GO1TK6nxSy \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step   \n",
    "  n is how many characters to predict\n",
    "  \"\"\"\n",
    "  #create vector\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  #customize it for our seed char\n",
    "  x[seed_ix] = 1\n",
    "  #list to store generated chars\n",
    "  ixes = []\n",
    "  #for as many characters as we want to generate\n",
    "  for t in range(n):\n",
    "    #a hidden state at a given time step is a function \n",
    "    #of the input at the same time step modified by a weight matrix \n",
    "    #added to the hidden state of the previous time step \n",
    "    #multiplied by its own hidden state to hidden state matrix.\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    #compute output (unnormalised)\n",
    "    y = np.dot(Why, h) + by\n",
    "    ## probabilities for next chars\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    #pick one with the highest probability \n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    #create a vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for the predicted char\n",
    "    x[ix] = 1\n",
    "    #add it to the list\n",
    "    ixes.append(ix)\n",
    "\n",
    "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "  print ('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  [67, 25, 12, 21, 46, 68, 8, 72, 18, 46, 8, 25, 12, 68, 40, 60, 46, 21, 28, 68, 67, 67, 54, 64, 8]\n",
      "targets:  [25, 12, 21, 46, 68, 8, 72, 18, 46, 8, 25, 12, 68, 40, 60, 46, 21, 28, 68, 67, 67, 54, 64, 8, 40]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print (\"inputs: \",inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print (\"targets: \",targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 108.268332\n",
      "----\n",
      " uZlRSkO5tKmit*,e“ONNZ!DZ0nC akVgG)AnGNddahEAxYH“EjAGVorBxyYZp4Tq:_3G”mgURPU'!3iKSCb g-E7DjUdJU)EFvZtcvgq!z9-e”0eH!rCd-!)“IvAVc6D“YAOc”lAGN0YGH9 fAI5”,Cf(;lWP63Unf-GhHq.hi97””I2“U OMrwi?:46S0,'V'h!P.wn \n",
      "----\n",
      "iter 1000, loss: 79.651707\n",
      "----\n",
      " P Mren, bwabind tabivt th hho wheiwhend ted thr bo wweso wind hisouy af ton sesayl. Buto,pissinitheest hasa fenteitheyonson hise, fang iticad am. Be ave whous re thed at Mrnintras qhet\n",
      "\n",
      "fa finptnhe, l \n",
      "----\n",
      "iter 2000, loss: 67.108263\n",
      "----\n",
      " oft.\n",
      "\n",
      "“I othas wanve maber whe the, Ble hheowevsethe sur ias thit” oxxmare oA ifhlas the, h; so Uthln basme he th her” th and hond ina My ef ifwart,mesy fryudre lo hind, wherte Mud an an. wang he dint \n",
      "----\n",
      "iter 3000, loss: 61.681601\n",
      "----\n",
      " otis wur'snelithery en herl,\n",
      "bd psoof on soemus pitcly id yuot pis iey”\n",
      "\n",
      "facd, and in shod_ repin, lurlril Therdolice thhe maraped ua\n",
      "pang I shintist by arod qfoacash Moowy, inimerd yh yo Ne hingu ues \n",
      "----\n",
      "iter 4000, loss: 59.539161\n",
      "----\n",
      " Thon y dar terinver, E,” qurled any bleaad nate foridchottithe ovethenwalg her the hirerelthurje towreich our ing comethe i.”-\n",
      "“I ad\n",
      "om?”\n",
      "\n",
      "“Wout endechinnce boude ther fotegs ance cay widiet'ed murest \n",
      "----\n",
      "iter 5000, loss: 58.646994\n",
      "----\n",
      "  hit poopil toul boweir to, noone, Barpirde dime Bertato fonled, Jathe gort ild wich. I\n",
      "treing serkslerens ipw hor semegtoy to wothaptice anasuld;\n",
      "no-\n",
      "wisat noun_ pingvo ex\n",
      "neng sind an. Shldees ouce  \n",
      "----\n",
      "iter 6000, loss: 57.804960\n",
      "----\n",
      " ”\n",
      "\n",
      "“Wh ifto vats histerled I the an herat of cry us onhitht horvernelpat. un vingOhue pond coure shill: “and bad I lidd wacg mI te hingiln oI baseca desgowis. Hord iist foangiele be vingee_gitjert Mr. \n",
      "----\n",
      "iter 7000, loss: 57.238044\n",
      "----\n",
      " eleveve iivo licfs wiygithanxenw atser,\n",
      "watiy bne\n",
      "suvey sousud, thespengragid\n",
      "mape hore lrre uel suctid ant yr ameg; hiis fres; aves tifhe hird e“suenfun pere5meotisled hot noneint\n",
      "\n",
      "as doto winn hont  \n",
      "----\n",
      "iter 8000, loss: 56.999750\n",
      "----\n",
      " t. Your fhan\n",
      "miug wer nr youh wint ainver s, on to\n",
      "ton\n",
      "of lecfit, Kmrow someacopurevy, la, dasu waty hed, mes reppisl tho bep aln, and\n",
      "remyd,\n",
      ",\n",
      "olip tot cfas re not thear hyturither co tos tome nouthe \n",
      "----\n",
      "iter 9000, loss: 56.825834\n",
      "----\n",
      " as the hlesued,\n",
      "dey;es ghichiy fone noughoren, soned hargiew cigh; of shiwsevece hat ur the thed githeir thonce of ofpriewquress\n",
      "cow. I iscy anoued\n",
      "wiof.”\n",
      "\n",
      "“Youngar withed h cuch in, rithitI ico howe  \n",
      "----\n",
      "iter 10000, loss: 56.693001\n",
      "----\n",
      " pedto tithy olpetocedive uthanacn, fothal red pfo oR't honh th hr, cfowest hory ebeper\n",
      "alrlesssighe wiseet hy il prece labursidutser inco\n",
      "thomiontme mene the sour high. I sheat. Irringave ofand pead a \n",
      "----\n",
      "iter 11000, loss: 56.343264\n",
      "----\n",
      " e ande, doid ro, ill en harcout woutsermean my wasiy veargr\n",
      "ace vive withy weatquet grepches hlettering yould as Whr prorech\n",
      "on\n",
      "rochisk no hichid fas. bont nos buing moult tho I ghes ow wars ig reives \n",
      "----\n",
      "iter 12000, loss: 56.189433\n",
      "----\n",
      " engo wee wast en hen tiom aplibedyeLinarodd. Mntheom asg\n",
      "awrseriss.\n",
      "Wis thotle her tor aind thinmothilve heogporet woressdide sonwute bed aevityenghe in\n",
      "and\n",
      "Elnlaming ch: hKeofrel meliinleanca nosliln \n",
      "----\n",
      "iter 13000, loss: 56.299828\n",
      "----\n",
      " I Deriogtint mun\n",
      "mud th to if itthitpeiteove louldroall yot war aid er sler id\n",
      ". Tong tom ansh\n",
      "in Eicge\n",
      "wiche thiof besine sumatat sitico thed sapuf\n",
      "date th an widlatt, hine soufof sho dilangers, so a \n",
      "----\n",
      "iter 14000, loss: 56.161036\n",
      "----\n",
      " iat haefine. You.\n",
      "Ekizy” gorint half\n",
      "rofom wofo ring\n",
      "intonecen\n",
      "nr. bepodllla\n",
      "taed liberedleram\n",
      "amd. Daboged him wowo ol me. Anencing mfererecussho and ovevad, bt int po retmy tringroutmisinudfiefdy, t \n",
      "----\n",
      "iter 15000, loss: 56.299691\n",
      "----\n",
      " izasdis the hon mitherbecef thilt-erent fon hily alxingion sedsaadat, waried dalavass her-,\n",
      "\n",
      "The 3opurind marre the lfiter, an\n",
      "dande, abiy ward en whed. Bug anne checfuch opuln sewlend rens Ir tas che \n",
      "----\n",
      "iter 16000, loss: 56.305026\n",
      "----\n",
      " hl, as ses harencest-ind to whesy seme womidy Repislery of soogh hite whrlets to hed\n",
      "er honghemsha sed chitad. Ture atsues thes theot the,\n",
      "of soung bice waed.”\n",
      "\n",
      "Heind to so hxcatidm fotlely part they  \n",
      "----\n",
      "iter 17000, loss: 56.326853\n",
      "----\n",
      " ico ally\n",
      "deponkeben, har akster, Ir wite nle; wire bushhat therilltcstence on ofe fornrenenan.\n",
      "\n",
      "Sort\n",
      "ind. bepat\n",
      "auln for hened, sawalppasuintteeg_ chion-y fer suscifhe; and ageof of ans wich rcharemen \n",
      "----\n",
      "iter 18000, loss: 55.627994\n",
      "----\n",
      " and tousy, Mastied wood, cr, of noke th, sotaso lid ig ande notile dast hlald ar _ofer inweryelerthen Couve, th\n",
      "bliskicasdeve thardeas\n",
      "suyivityed shith efo id when, Withes en fandonusurle pow, an, her \n",
      "----\n",
      "iter 19000, loss: 55.275104\n",
      "----\n",
      " d nr ridist as merithy wa\n",
      "that of mat am, wnise nr ked, cunmifusped sfon pored hy beken weeogamerbeint an fres fom oh eelelithens datcer nrasJey cofsel infuss she sy er hid all she thinned er; ofed co \n",
      "----\n",
      "iter 20000, loss: 56.429956\n",
      "----\n",
      "  bussefper angrald sase Daed whad, and,, oCafoum Bdice as Werlithenn\n",
      "think the fie.,\n",
      "“Be cane; abevit weno\n",
      "her. beurlm ald pou Here.”\n",
      "NM rmer oned prer by en da rasto cuvine thethis ed onirco bonteL h \n",
      "----\n",
      "iter 21000, loss: 56.275309\n",
      "----\n",
      " en the\n",
      "tetjin and cas dathed ler, sopel ich am\n",
      "dabeey paren cumy pathaveritalf\n",
      "ivig1elighine sitice be herat ogh..\n",
      "Id at the in anly tost who titucoug ter.d\n",
      "Notta fred con to cain.;\n",
      "“Un; hasvecd womir \n",
      "----\n",
      "iter 22000, loss: 56.583201\n",
      "----\n",
      " ao.\n",
      "\n",
      "“coth it they; in cins. Winl seodithenle pered\n",
      "'ed and bpun ses surkereN sery. Im har cot ul ceaet arrsen ute astene bew tof os-wing woutghle\n",
      "1ures sfanter mrled; an sen woul be,--Noded. Loaced a \n",
      "----\n",
      "iter 23000, loss: 55.897992\n",
      "----\n",
      " \n",
      "Naps ly, has hatceren gebhaw the fer. Id pel-vigh weriel serl on thaaed\n",
      "met the\n",
      "shid, wacl\n",
      "tok on\n",
      "wrhe wieg imuttidoncomiondury inmleine\n",
      "poeveceibit; Jlleld Darl sonewo fouk. My dapilk\n",
      "h_\n",
      "de coue\n",
      "the \n",
      "----\n",
      "iter 24000, loss: 55.813736\n",
      "----\n",
      " lich, ha, whjo denneivems Jascus ilf sar fant atlas in. Mach honen no you gorseof end dyal?”\n",
      "\n",
      "Theid the tichor hies wenced to bobershirdinn to to dased it ser. Shamridus one,\n",
      "or and one wiey\n",
      "_ratend a \n",
      "----\n",
      "iter 25000, loss: 55.490788\n",
      "----\n",
      " \n",
      "\n",
      "vers carmbat he dleter shewa mu ave the yoto the -suntiis whad yovever buit owagh\n",
      "woudgeyKis waskeng sele bey and the mastesbe meus bun it in cone har, ponge farcerber romy hun the and, at pief Memi \n",
      "----\n",
      "iter 26000, loss: 55.728890\n",
      "----\n",
      " erer the\n",
      "the fakle. Shir and to har sur nod em donoct mailt, thithe rifuonsthe pimicury fenec. Hameato cited fory\n",
      "th men verce linh spile ele rs ie. The benem ard ne cyin peos. A\n",
      "cout nom calcimentret \n",
      "----\n",
      "iter 27000, loss: 55.753663\n",
      "----\n",
      "  rourfiverly ards be wiof. Hink on hy mino leple tave hot surene to tetan Lise Lite Fang h;n to cettounctio haml thed, ill,\n",
      "izlid may so the no. But dereed por!,\n",
      "Ey\n",
      "worting nowk me. I -take, bet?” bou \n",
      "----\n",
      "iter 28000, loss: 55.770620\n",
      "----\n",
      "  iHe chit gorjle ar.”\n",
      "\n",
      "“Younefer thy buny.\n",
      "\n",
      "“It to at; at at, and thittert to sindiln Ry; ofial the\n",
      "tocline he cearl ye swap hich hy fer gous gey hurs sas hus. zatrof..” _re finkigh heomher mald “ropy \n",
      "----\n",
      "iter 29000, loss: 55.849386\n",
      "----\n",
      " and key nes hey mer Nein\n",
      "wabing. Dionel, from hewt.\n",
      "”r yous fare sisne to cray. Bus wole the tho gextare hor to landing\n",
      "Bimiengr, alr reas Lit in sins ly! Bom mowmerley\n",
      "dey thereo\n",
      "dos sone hat mererel \n",
      "----\n",
      "iter 30000, loss: 55.370503\n",
      "----\n",
      " ndeaptart wrailcles.\n",
      "\n",
      "“Bpret muth\n",
      "not waln and mat sees. eOk hild tharyigh it faps sitesteepy Mrend aill of Ekand.\n",
      "\n",
      "“I hepisure, whid dofiom amapprith ies Cos noll tho sere Da of Mes of yout lineche.  \n",
      "----\n",
      "iter 31000, loss: 55.460028\n",
      "----\n",
      " wound bouns maln. if\n",
      "rald and\n",
      "shet thes moneed\n",
      "or.\n",
      "\n",
      "“Andling serpiis\n",
      "Ela\n",
      "izathowy, ins with\n",
      "incill”\n",
      "\n",
      "ofth; and the ilf ach come when and arny nod to toecwad thent us teftey the of woppy wit ene be ace \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d9e2ca2d4ca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-5103d04ee76b>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m \u001b[0;31m# unnormalized log probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# softmax (cross-entropy loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;31m# backward pass: compute gradients going backwards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2060\u001b[0m     \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m     \"\"\"\n\u001b[0;32m-> 2062\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_gentype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2063\u001b[0m         \u001b[0;31m# 2018-02-25, 1.15.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m         warnings.warn(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*150:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "  if p+seq_length+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data                                                                                                                                                             \n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "  if n % 1000 == 0:\n",
    "    print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "  p += seq_length # move data pointer                                                                                                                                                         \n",
    "  n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
